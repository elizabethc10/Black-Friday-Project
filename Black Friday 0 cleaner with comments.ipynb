{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practice problem from Analytics Vidhya. Predicting customers' purchase amounts from information about customer, products, and\n",
    "# past purchases.\n",
    "\n",
    "#Incomplete\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "#from sklearn.ensemble import VotingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "#Section A: preprocessing\n",
    "\n",
    "#lots of different parts take a long time, so this function is for keeping track of how long parts take so I can balance time\n",
    "# vs. thoroughness\n",
    "times = []\n",
    "tstart = time.time()\n",
    "def how_long(t_st):\n",
    "    t1 = time.time()\n",
    "    td = t1 - t_st\n",
    "    times.append(td)\n",
    "    print('Part {} time {}'.format(len(times), td))\n",
    "    return t1\n",
    "\n",
    "#putting data in dataframes\n",
    "trainRaw = pd.read_csv('train.csv')\n",
    "testRaw = pd.read_csv('test.csv')\n",
    "train = trainRaw.copy()\n",
    "test = testRaw.copy()\n",
    "\n",
    "# print(train.head())\n",
    "# print(test.head())\n",
    "\n",
    "#preprocessing function\n",
    "def preproc(df):\n",
    "    #replace '4+' with 4 to make all Stay in City values numeric\n",
    "    df['Stay_In_Current_City_Years'].replace('4+', 4, inplace=True)\n",
    "    \n",
    "    #convert Product Category data to strings and replace \"nan\" with \"missing\"\n",
    "    missing = df[['Product_Category_1', 'Product_Category_2', 'Product_Category_3']]\n",
    "    missing = missing.applymap(str)\n",
    "    missing = missing.replace('nan', 'missing')\n",
    "    df[['Product_Category_1', 'Product_Category_2', 'Product_Category_3']] = missing\n",
    "    \n",
    "    #use ordinal encoder on Gender and Age columns\n",
    "    ages = ['0-17', '18-25', '26-35', '36-45', '46-50', '51-55', '55+']\n",
    "    mapAge = list(zip(ages, range(0,7)))\n",
    "    dictGend = {'col': 'Gender', 'mapping': [('M', 0), ('F', 1)]}\n",
    "    oe = ce.OrdinalEncoder(cols=['Gender', 'Age'], mapping=[dictGend, {'col': 'Age', 'mapping': mapAge}])\n",
    "    df = oe.fit_transform(df)\n",
    "    \n",
    "    #use binary encoder on categorical features that should not be ordered\n",
    "    catsNotOrd = ['User_ID', 'Product_ID', 'Occupation', 'City_Category', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3']\n",
    "    ceb = ce.BinaryEncoder(cols=catsNotOrd)\n",
    "    df = ceb.fit_transform(df)\n",
    "    \n",
    "    #minmax scale Age\n",
    "    scale = MinMaxScaler()\n",
    "    df[['Age', 'Stay_In_Current_City_Years']] = scale.fit_transform(df[['Age', 'Stay_In_Current_City_Years']].values)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#perform preprocessing\n",
    "train = preproc(train)\n",
    "test = preproc(test)\n",
    "\n",
    "#separate training data into predictive and target variables\n",
    "x = train.drop('Purchase', axis=1)\n",
    "y = train['Purchase']\n",
    "\n",
    "# Trying not using some features that seem to be less important\n",
    "few_feat = ['Product_ID_1', 'Product_ID_2', 'Product_ID_3', 'Product_ID_4',\n",
    "       'Product_ID_5', 'Product_ID_7', 'Product_ID_8', 'Product_ID_9',\n",
    "       'City_Category_2', 'Product_Category_1_1', 'Product_Category_1_2',\n",
    "       'Product_Category_1_3', 'Product_Category_1_4', 'Product_Category_1_5',\n",
    "       'Age', 'Stay_In_Current_City_Years']\n",
    "x0 = x.copy()\n",
    "test0 = test.copy()\n",
    "x1 = x[few_feat]\n",
    "test1 = test[few_feat]\n",
    "\n",
    "#train-test split\n",
    "xtr0, xtest0, ytr, ytest = train_test_split(x0, y, random_state=2)\n",
    "xtr1, xtest1, ytr, ytest = train_test_split(x1, y, random_state=2)\n",
    "\n",
    "#putting different versions of data into lists together\n",
    "lxtr = [xtr0, xtr1]\n",
    "lxtest = [xtest0, xtest1]\n",
    "\n",
    "#timer function\n",
    "endtime = how_long(tstart)\n",
    "\n",
    "\n",
    "#some visualization\n",
    "# f, axes = plt.subplots(3,3, figsize=(15,15))\n",
    "# colCat = ['Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status',\n",
    "#           'Product_Category_1', 'Product_Category_2', 'Product_Category_3']\n",
    "# for a in range(0,9):\n",
    "#     sns.violinplot(x=colCat[a], y='Purchase', data=trainRaw, ax=axes[int(a/3),a%3])\n",
    "#\n",
    "# f, ax = plt.subplots()\n",
    "# sns.heatmap(trainRaw.corr())\n",
    "\n",
    "# g = sns.PairGrid(trainRaw, y_vars=\"Purchase\",\n",
    "#                  x_vars=['Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years'])\n",
    "# g.map(sns.barplot)\n",
    "\n",
    "# g = sns.PairGrid(trainRaw, y_vars=\"Purchase\",\n",
    "#                  x_vars=['Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3'])\n",
    "# g.map(sns.barplot)\n",
    "\n",
    "\n",
    "#Section B: building and tuning models\n",
    "\n",
    "#record start time\n",
    "tstart = time.time()\n",
    "\n",
    "#function for calculating Root Mean Squares Logarithmic Error\n",
    "def rmsle(y_targ, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_targ, y_pred))\n",
    "\n",
    "#some regressor models\n",
    "rfr = RandomForestRegressor(n_estimators=20, max_depth=25, min_samples_split=0.4, max_leaf_nodes=50,\n",
    "                            max_features=0.3, random_state=2, n_jobs=5, min_samples_leaf=0.15)\n",
    "sgdr = linear_model.SGDRegressor(max_iter=7, random_state=2, alpha=1e-05)\n",
    "gbr = GradientBoostingRegressor(learning_rate=0.2, min_samples_leaf=50, max_features='sqrt',\n",
    "                                subsample=0.8, random_state=2, n_estimators=40, min_samples_split=0.6, max_depth=2)\n",
    "xgbr = xgb.XGBRegressor(random_state=2, subsample=0.7, max_depth=7)\n",
    "\n",
    "#list with models and copies to use on version of data with fewer features\n",
    "models = [[rfr, sgdr, gbr, xgbr]]\n",
    "models.append([copy.copy(m) for m in models[0]])\n",
    "\n",
    "#trying to fit and score models and time how long fitting and scoring take in a systematic way\n",
    "\n",
    "# bm = models.copy()\n",
    "# bstr = [0,0,0,0]\n",
    "# bste = [0,0,0,0]\n",
    "# for m in range(4):\n",
    "#     bm[m].fit(X_train, y_train)\n",
    "#     bstr[m] = bm[m].score(X_train, y_train)\n",
    "#     bste[m] = bm[m].score(X_test, y_test)\n",
    "#     print(bstr[m])\n",
    "#     print(bste[m])\n",
    "\n",
    "# scores=[0,0,0,0]\n",
    "# times = [0,0,0,0]\n",
    "    \n",
    "# def eval_score(m):\n",
    "#     scores[m] = rmse_cv(models[m]).mean()\n",
    "#     print(scores[m])\n",
    "    \n",
    "# for i in range(4):\n",
    "#     times[i] = timeit.timeit('eval_score(i)', number=1, globals=globals())\n",
    "#     print(times[i])\n",
    "\n",
    "#parameters to try using Randomized Search\n",
    "rfr_param_grid = {'min_samples_leaf' : np.arange(0.05, 0.36, 0.05)}\n",
    "sgdr_param_grid = {'alpha': 10.0**-np.arange(1,7)}\n",
    "gbr_param_grid = {'n_estimators' : range(10, 61, 10),\n",
    "                  'max_depth' : range(2, 8),\n",
    "                 'min_samples_split' : np.arange(0.5, 0.95, 0.05)}\n",
    "xgbr_param_grid = {'max_depth' : [3, 5, 6, 7],\n",
    "                  'subsample' : np.arange(0.4, 1, 0.1)}\n",
    "\n",
    "#variables for systematically using Randomized Search on different models and data versions\n",
    "#s_train = [0,0,0,0]\n",
    "#s_test = [0,0,0,0]\n",
    "par_grids = [rfr_param_grid, sgdr_param_grid, gbr_param_grid, xgbr_param_grid]\n",
    "best_models = pd.DataFrame(index=range(4), columns=range(2))\n",
    "predict_tr = best_models.copy()\n",
    "predict_test = best_models.copy()\n",
    "\n",
    "#functions for creating Randomized Search object, fitting, and using to make predictions\n",
    "def makegs(model_mgs, pg_mgs):\n",
    "    return RandomizedSearchCV(model_mgs, param_distributions = pg_mgs, n_iter=3, cv=3, n_jobs=5, random_state=2)\n",
    "\n",
    "def fitgs(gsfgs, xfgs, yfgs):\n",
    "    gsfgs.fit(xfgs, yfgs)\n",
    "    return gsfgs\n",
    "\n",
    "def predbm(bmpbm, xtrpbm, xtestpbm):\n",
    "    ptr = bmpbm.predict(xtrpbm)\n",
    "    ptest = bmpbm.predict(xtestpbm)\n",
    "    return ptr, ptest\n",
    "\n",
    "#timer function\n",
    "tstart = how_long(tstart)\n",
    "\n",
    "#i=1 is for fewer features set, not i=0\n",
    "# for j in range(1,4):\n",
    "#     i=0\n",
    "#     gsearch = makegs(models[j], par_grids[j])\n",
    "#     gsearch = fitgs(gsearch, lxtr[i], ytr)\n",
    "#     tstart = how_long(tstart)\n",
    "#     print(gsearch.best_params_)\n",
    "#     for i in range(2):\n",
    "#         best_models.iloc[j, i] = gsearch.best_estimator_\n",
    "\n",
    "#fitting best models from Randomized Search to different versions of data\n",
    "for j in range(4):\n",
    "    for i in range(2):\n",
    "        best_models.iloc[j, i] = models[i][j]\n",
    "        best_models.iloc[j, i].fit(lxtr[i], ytr)\n",
    "        tstart = how_long(tstart)\n",
    "\n",
    "#making predictions using fitted models\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        prtr, prtest = predbm(best_models.iloc[j, i], lxtr[i], lxtest[i])\n",
    "        print(prtr)\n",
    "        print(prtest)\n",
    "        predict_tr.iloc[j, i] = prtr\n",
    "        predict_test.iloc[j, i] = prtest\n",
    "        tstart = how_long(tstart)\n",
    "\n",
    "#trying another way of doing the two previous loops\n",
    "# for i in range(2):\n",
    "#     for j in range(4):\n",
    "#         gsearch = makegs(models[j], par_grids[j])\n",
    "#         gsearch = fitgs(gsearch, lxtr[i], ytr)\n",
    "#         tstart = how_long(tstart)\n",
    "#         best_models.iloc[j, i] = gsearch.best_estimator_\n",
    "#         print(gsearch.best_params_)\n",
    "#         prtr, prtest = predbm(best_e, lxtr[i], lxtest[i])\n",
    "#         print(prtr)\n",
    "#         print(prtest)\n",
    "#         predict_tr.iloc[j, i] = prtr\n",
    "#         predict_test.iloc[j, i] = prtest\n",
    "#         tstart = how_long(tstart)\n",
    "\n",
    "\n",
    "\n",
    "#Section C: RMSE CV scores\n",
    "\n",
    "#timer function\n",
    "tstart = time.time()\n",
    "\n",
    "#function for Cross-Validated Root Mean Squares Error\n",
    "def rmse_cv(model, x_rmsecv, y_rmsecv):\n",
    "    rmse = np.sqrt(-cross_val_score(model, x_rmsecv, y_rmsecv, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)\n",
    "\n",
    "#empty dataframe for RMSE scores\n",
    "scores_rmsecv = pd.DataFrame(index=range(4), columns=range(2))\n",
    "\n",
    "#RMSE scores of only the models that used the data with fewer features\n",
    "print('RMSE CV scores, fewer features models only')\n",
    "for j in range(4):\n",
    "    scores_rmsecv[1][j] = rmse_cv(best_models[1][j], xtr1, ytr)\n",
    "    print(scores_rmsecv[1][j])\n",
    "\n",
    "#timer function\n",
    "endtime = how_long(tstart)\n",
    "\n",
    "\n",
    "#Section D: model scores and RMSLE scores\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "#empty dataframes for scores\n",
    "tr_scores_m = pd.DataFrame(index=range(4), columns=range(2))\n",
    "tr_scores_rmsle = tr_scores_m.copy()\n",
    "test_scores_m = tr_scores_m.copy()\n",
    "test_scores_rmsle = tr_scores_m.copy()\n",
    "\n",
    "#function for getting scores using built-in score function and RMSLE\n",
    "def scores(model_sc, x_sc, y_sc, predict_sc):\n",
    "    score_m = model_sc.score(x_sc, y_sc)\n",
    "    score_rmsle = rmsle(y_sc, predict_sc)\n",
    "    return score_m, score_rmsle\n",
    "\n",
    "#printing results\n",
    "for i in range(2):\n",
    "    print('Set {}'.format(i))\n",
    "    for j in range(4):\n",
    "        tr_scores_m[i][j], tr_scores_rmsle[i][j] = scores(best_models[i][j], lxtr[i], ytr, predict_tr[i][j])\n",
    "        test_scores_m[i][j], test_scores_rmsle[i][j] = scores(best_models[i][j], lxtest[i], ytest, predict_test[i][j])\n",
    "        print('Model {}'.format(j))\n",
    "        print('Training {}, {}'.format(tr_scores_m[i][j], tr_scores_rmsle[i][j]))\n",
    "        print('Test {}, {}'.format(test_scores_m[i][j], test_scores_rmsle[i][j]))\n",
    "\n",
    "endtime = how_long(tstart)\n",
    "\n",
    "\n",
    "#Section E: feature importances\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "#averages of feature importances\n",
    "def avpr_featimp(smodels, colnames):\n",
    "    df_f_imp = pd.DataFrame(index=colnames, columns=[0, 2, 3])\n",
    "    feat_imp = [m.feature_importances_ for m in smodels]\n",
    "    \n",
    "    df_f_imp.iloc[:] = np.transpose(feat_imp)\n",
    "    df_f_imp['average'] = df_f_imp.mean(axis=1)\n",
    "    print(df_f_imp)\n",
    "    print(df_f_imp['average'].loc[df_f_imp['average']>0.01])\n",
    "    return df_f_imp\n",
    "\n",
    "print('Feature importances')\n",
    "cm0 = best_models[0].iloc[[0, 2, 3]]\n",
    "f_imp0 = avpr_featimp(cm0, x0.columns)\n",
    "cm1 = best_models[1].iloc[[0, 2, 3]]\n",
    "f_imp1 = avpr_featimp(cm1, x1.columns)\n",
    "\n",
    "endtime = how_long(tstart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
