{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incomplete\n",
    "\n",
    "#remaining stuff to do:\n",
    "#-implement pipelines to test using all vs. fewer features and Random Forest Regressor vs. Stacked Average model\n",
    "#-add comments\n",
    "#-clean up unused parts\n",
    "#-maybe figure out other ways to improve score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "#from sklearn.ensemble import VotingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "times = []\n",
    "ttt = time.time()\n",
    "def tt():\n",
    "    t1 = time.time()\n",
    "    td = t1 - ttt\n",
    "    times.append(td)\n",
    "    print('Part {} time {}'.format(len(times), td))\n",
    "    ttt = t1\n",
    "\n",
    "trainRaw = pd.read_csv('train.csv')\n",
    "testRaw = pd.read_csv('test.csv')\n",
    "train = trainRaw.copy()\n",
    "test = testRaw.copy()\n",
    "\n",
    "# print(train.head())\n",
    "# print(test.head())\n",
    "\n",
    "def preproc(df):\n",
    "    #replace '4+' with 4 to make all Stay in City values numeric\n",
    "    df['Stay_In_Current_City_Years'].replace('4+', 4, inplace=True)\n",
    "    \n",
    "    missing = df[['Product_Category_1', 'Product_Category_2', 'Product_Category_3']]\n",
    "    missing = missing.applymap(str)\n",
    "    missing = missing.replace('nan', 'missing')\n",
    "    df[['Product_Category_1', 'Product_Category_2', 'Product_Category_3']] = missing\n",
    "    \n",
    "    #use ordinal encoder on Gender and Age columns\n",
    "    ages = ['0-17', '18-25', '26-35', '36-45', '46-50', '51-55', '55+']\n",
    "    mapAge = list(zip(ages, range(0,7)))\n",
    "    dictGend = {'col': 'Gender', 'mapping': [('M', 0), ('F', 1)]}\n",
    "    oe = ce.OrdinalEncoder(cols=['Gender', 'Age'], mapping=[dictGend, {'col': 'Age', 'mapping': mapAge}])\n",
    "    df = oe.fit_transform(df)\n",
    "    \n",
    "    #use binary encoder on categorical features that should not be ordered\n",
    "    catsNotOrd = ['User_ID', 'Product_ID', 'Occupation', 'City_Category', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3']\n",
    "    ceb = ce.BinaryEncoder(cols=catsNotOrd)\n",
    "    df = ceb.fit_transform(df)\n",
    "    \n",
    "    #minmax scale Age\n",
    "    scale = MinMaxScaler()\n",
    "    df[['Age', 'Stay_In_Current_City_Years']] = scale.fit_transform(df[['Age', 'Stay_In_Current_City_Years']].values)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = preproc(train)\n",
    "test = preproc(test)\n",
    "\n",
    "dfx = train.drop('Purchase', axis=1)\n",
    "dfy = train['Purchase']\n",
    "\n",
    "# Trying not using some features that seem to be less important\n",
    "few_feat = ['Product_ID_1', 'Product_ID_2', 'Product_ID_3', 'Product_ID_4',\n",
    "       'Product_ID_5', 'Product_ID_7', 'Product_ID_8', 'Product_ID_9',\n",
    "       'City_Category_2', 'Product_Category_1_1', 'Product_Category_1_2',\n",
    "       'Product_Category_1_3', 'Product_Category_1_4', 'Product_Category_1_5',\n",
    "       'Age', 'Stay_In_Current_City_Years']\n",
    "x0 = dfx.copy()\n",
    "test0 = test.copy()\n",
    "x1 = dfx[few_feat]\n",
    "test1 = test[few_feat]\n",
    "\n",
    "#train-test split\n",
    "xtr0, xtest0, ytr0, ytest0 = train_test_split(x0, dfy, random_state=2)\n",
    "xtr1, xtest1, ytr1, ytest1 = train_test_split(x1, dfy, random_state=2)\n",
    "\n",
    "tt()\n",
    "\n",
    "#some visualization\n",
    "# f, axes = plt.subplots(3,3, figsize=(15,15))\n",
    "# colCat = ['Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status',\n",
    "#           'Product_Category_1', 'Product_Category_2', 'Product_Category_3']\n",
    "# for a in range(0,9):\n",
    "#     sns.violinplot(x=colCat[a], y='Purchase', data=trainRaw, ax=axes[int(a/3),a%3])\n",
    "#\n",
    "# f, ax = plt.subplots()\n",
    "# sns.heatmap(trainRaw.corr())\n",
    "\n",
    "# g = sns.PairGrid(trainRaw, y_vars=\"Purchase\",\n",
    "#                  x_vars=['Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years'])\n",
    "# g.map(sns.barplot)\n",
    "\n",
    "# g = sns.PairGrid(trainRaw, y_vars=\"Purchase\",\n",
    "#                  x_vars=['Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3'])\n",
    "# g.map(sns.barplot)\n",
    "\n",
    "rfr = RandomForestRegressor(random_state=2, n_estimators=120, n_jobs=-1)\n",
    "sgdr = linear_model.SGDRegressor(random_state=2)\n",
    "gbr = GradientBoostingRegressor(random_state=2)\n",
    "xgbr = xgb.XGBRegressor(random_state=2)\n",
    "\n",
    "models=[rfr, sgdr, gbr, xgbr]\n",
    "\n",
    "# bm = models.copy()\n",
    "# bstr = [0,0,0,0]\n",
    "# bste = [0,0,0,0]\n",
    "# for m in range(4):\n",
    "#     bm[m].fit(X_train, y_train)\n",
    "#     bstr[m] = bm[m].score(X_train, y_train)\n",
    "#     bste[m] = bm[m].score(X_test, y_test)\n",
    "#     print(bstr[m])\n",
    "#     print(bste[m])\n",
    "\n",
    "# def rmse_cv(model):\n",
    "#     rmse = np.sqrt(-cross_val_score(model, dfx, dfy, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "#     return(rmse)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "# scores=[0,0,0,0]\n",
    "# times = [0,0,0,0]\n",
    "    \n",
    "# def eval_score(m):\n",
    "#     scores[m] = rmse_cv(models[m]).mean()\n",
    "#     print(scores[m])\n",
    "    \n",
    "# for i in range(4):\n",
    "#     times[i] = timeit.timeit('eval_score(i)', number=1, globals=globals())\n",
    "#     print(times[i])\n",
    "    \n",
    "rfr_param_grid = {'max_features' : ['auto', 'sqrt', 'log2']}\n",
    "sgdr_param_grid = {'loss' : ['squared_loss', 'huber'],\n",
    "                 'penalty' : ['l2', 'l1'],\n",
    "                  'alpha': [0.001, 0.0001, 0.00001]}\n",
    "gbr_param_grid = {'max_depth' : [5, 7, 9]}\n",
    "xgbr_param_grid = {'max_depth' : [4, 6],\n",
    "                  'subsample' : [0.6, 0.8]}\n",
    "\n",
    "#s_train = [0,0,0,0]\n",
    "#s_test = [0,0,0,0]\n",
    "pgrids = [rfr_param_grid, sgdr_param_grid, gbr_param_grid, xgbr_param_grid]\n",
    "rs = np.zeros(4, 2)\n",
    "best_estimators = np.zeros(4, 2)\n",
    "b_scores = np.zeros(4, 2)\n",
    "b_rmsle = np.zeros(4, 2)\n",
    "pr_tr = np.zeros(4, 2)\n",
    "\n",
    "def r_search(m):\n",
    "    rs[m] = RandomizedSearchCV(models[m], param_distributions = pgrids[m], cv=5, n_jobs=-1)\n",
    "    rs[m].fit(dfx, dfy)\n",
    "    best_estimators[m] = rs[m].best_estimator_\n",
    "    pr_tr[m] = rs[m].predict(dfx)\n",
    "    \n",
    "def best_score(m):\n",
    "#     s_train[m] = best_estimators[m].score(X_train, y_train)\n",
    "#     s_test[m] = best_estimators[m].score(X_test, y_test)\n",
    "    b_scores[m] = best_estimators[m].score(dfx, dfy)\n",
    "#     print(s_train[m])\n",
    "#     print(s_test[m])\n",
    "    print(scores[m])\n",
    "    \n",
    "def best_rmsle(m):\n",
    "    b_rmsle[m] = rmsle(dfy, pr_tr[m])\n",
    "    print(b_rmsle[m])\n",
    "    \n",
    "for i in range(4):\n",
    "    print(i)\n",
    "    r_search(i)\n",
    "    best_score(m)\n",
    "    best_rmsle(i)\n",
    "    \n",
    "feat_imp = pd.DataFrame(index=dfx.columns)\n",
    "feat_imp['a'] = best_estimators[0].feature_importances_\n",
    "feat_imp['b'] = best_estimators[2].feature_importances_\n",
    "feat_imp['c'] = best_estimators[3].feature_importances_\n",
    "feat_imp['av'] = feat_imp.mean(axis=1)\n",
    "print('Feature importances')\n",
    "print(feat_imp.loc[feat_imp['av']>0.005].index)\n",
    "    \n",
    "#copied from Kaggle kernel \"Stacked Regressions to predict House Prices\", by Serigne\n",
    "\n",
    "# class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "#     def __init__(self, models):\n",
    "#         self.models = models\n",
    "        \n",
    "#     # we define clones of the original models to fit the data in\n",
    "#     def fit(self, X, y):\n",
    "#         self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "#         # Train cloned base models\n",
    "#         for model in self.models_:\n",
    "#             model.fit(X, y)\n",
    "\n",
    "#         return self\n",
    "    \n",
    "#     #Now we do the predictions for cloned models and average them\n",
    "#     def predict(self, X):\n",
    "#         predictions = np.column_stack([\n",
    "#             model.predict(X) for model in self.models_\n",
    "#         ])\n",
    "#         return np.mean(predictions, axis=1)\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "                y_pred = instance.predict(X.iloc[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n",
    "#end of copied code\n",
    "\n",
    "# averaged_models = AveragingModels(models = (bests[0], bests[2], bests[3]))\n",
    "# averaged_models.fit(X_train, y_train)\n",
    "# pred_av = averaged_models.predict(X_train)\n",
    "# score_average = rmsle(y_train, pred_av)\n",
    "# print('Average:')\n",
    "# print(score_average)\n",
    "\n",
    "# test_pr_av = averaged_models.predict(X_test)\n",
    "# score_test_av = rmsle(y_test, test_pr_av)\n",
    "# print(score_test_av)\n",
    "\n",
    "stacked_averaged_models = StackingAveragedModels(base_models = (best_estimators[0], best_estimators[2]),\n",
    "                                                 meta_model = best_estimators[3])\n",
    "# stacked_averaged_models.fit(X_train, y_train)\n",
    "# pred_st_av = stacked_averaged_models.predict(X_train)\n",
    "# score_st_av = rmsle(y_train, pred_st_av)\n",
    "# print('Stacked average:')\n",
    "# print(score_st_av)\n",
    "\n",
    "# test_pr_st_av = stacked_averaged_models.predict(X_test)\n",
    "# score_test_st_av = rmsle(y_test, test_pr_st_av)\n",
    "# print(score_test_st_av)\n",
    "\n",
    "stacked_averaged_models.fit(dfx, dfy)\n",
    "\n",
    "pred_st_av = stacked_averaged_models.predict(dfx)\n",
    "\n",
    "score_st_av = rmsle(dfy, pred_st_av)\n",
    "print('Stacked average:')\n",
    "print(score_st_av)\n",
    "\n",
    "# predictions_f = stacked_averaged_models.predict(test)\n",
    "# print(predictions_f)\n",
    "\n",
    "# df_s = pd.DataFrame(data=testRaw[['User_ID', 'Product_ID']], index=test.index)\n",
    "# df_s['Purchase'] = predictions_f\n",
    "# print(df_s)\n",
    "# df_s.to_csv('bfsubmission1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
