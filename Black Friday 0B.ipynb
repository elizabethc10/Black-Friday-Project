{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incomplete\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "#from sklearn.ensemble import VotingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "#Section A: preprocessing\n",
    "\n",
    "#function to check times at different points in code\n",
    "times = []\n",
    "tstart = time.time()\n",
    "def how_long(t_st):\n",
    "    t1 = time.time()\n",
    "    td = t1 - t_st\n",
    "    times.append(td)\n",
    "    print('Part {} time {}'.format(len(times), td))\n",
    "    return t1\n",
    "\n",
    "#reading data files\n",
    "trainRaw = pd.read_csv('train.csv')\n",
    "testRaw = pd.read_csv('test.csv')\n",
    "train = trainRaw.copy()\n",
    "test = testRaw.copy()\n",
    "\n",
    "# print(train.head())\n",
    "# print(test.head())\n",
    "\n",
    "#preprocessing\n",
    "def preproc(df):\n",
    "    #replace '4+' with 4 to make all Stay in City values numeric\n",
    "    df['Stay_In_Current_City_Years'].replace('4+', 4, inplace=True)\n",
    "    \n",
    "    missing = df[['Product_Category_1', 'Product_Category_2', 'Product_Category_3']]\n",
    "    missing = missing.applymap(str)\n",
    "    missing = missing.replace('nan', 'missing')\n",
    "    df[['Product_Category_1', 'Product_Category_2', 'Product_Category_3']] = missing\n",
    "    \n",
    "    #use ordinal encoder on Gender and Age columns\n",
    "    ages = ['0-17', '18-25', '26-35', '36-45', '46-50', '51-55', '55+']\n",
    "    mapAge = list(zip(ages, range(0,7)))\n",
    "    dictGend = {'col': 'Gender', 'mapping': [('M', 0), ('F', 1)]}\n",
    "    oe = ce.OrdinalEncoder(cols=['Gender', 'Age'], mapping=[dictGend, {'col': 'Age', 'mapping': mapAge}])\n",
    "    df = oe.fit_transform(df)\n",
    "    \n",
    "    #use binary encoder on categorical features that should not be ordered\n",
    "    catsNotOrd = ['User_ID', 'Product_ID', 'Occupation', 'City_Category', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3']\n",
    "    ceb = ce.BinaryEncoder(cols=catsNotOrd)\n",
    "    df = ceb.fit_transform(df)\n",
    "    \n",
    "    #minmax scale Age\n",
    "    scale = MinMaxScaler()\n",
    "    df[['Age', 'Stay_In_Current_City_Years']] = scale.fit_transform(df[['Age', 'Stay_In_Current_City_Years']].values)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = preproc(train)\n",
    "test = preproc(test)\n",
    "\n",
    "#making target and feature data sets\n",
    "x = train.drop('Purchase', axis=1)\n",
    "y = train['Purchase']\n",
    "\n",
    "# Trying not using some features that seem to be less important\n",
    "few_feat = ['Product_ID_1', 'Product_ID_2', 'Product_ID_3', 'Product_ID_4',\n",
    "       'Product_ID_5', 'Product_ID_7', 'Product_ID_8', 'Product_ID_9',\n",
    "       'City_Category_2', 'Product_Category_1_1', 'Product_Category_1_2',\n",
    "       'Product_Category_1_3', 'Product_Category_1_4', 'Product_Category_1_5',\n",
    "       'Age', 'Stay_In_Current_City_Years']\n",
    "x0 = x.copy()\n",
    "test0 = test.copy()\n",
    "x1 = x[few_feat]\n",
    "test1 = test[few_feat]\n",
    "\n",
    "#train-test split\n",
    "xtr0, xtest0, ytr, ytest = train_test_split(x0, y, random_state=2)\n",
    "xtr1, xtest1, ytr, ytest = train_test_split(x1, y, random_state=2)\n",
    "\n",
    "lxtr = [xtr0, xtr1]\n",
    "lxtest = [xtest0, xtest1]\n",
    "\n",
    "endtime = how_long(tstart)\n",
    "\n",
    "\n",
    "#Section B part 1 version 3: building and tuning models\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "#root mean squared log error evaluation function\n",
    "def rmsle(y_targ, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_targ, y_pred))\n",
    "\n",
    "#starting with Random Forest Regressor, tuning n_estimators\n",
    "rfr0 = RandomForestRegressor(max_depth=16, min_samples_split=10.0**(-4), max_features=0.8, max_leaf_nodes=None, random_state=2,\n",
    "                             min_samples_leaf=1, n_jobs=5)\n",
    "\n",
    "rfr_par_grid_n_est0 = {'n_estimators' : range(110, 141, 5)}\n",
    "\n",
    "gridsearchcv_rfr_n_est = GridSearchCV(rfr0, param_grid=rfr_par_grid_n_est0, cv=5, n_jobs=5)\n",
    "gridsearchcv_rfr_n_est.fit(xtr0, ytr)\n",
    "res_rfrnest = gridsearchcv_rfr_n_est.cv_results_\n",
    "best_est_rfrnest = gridsearchcv_rfr_n_est.best_estimator_\n",
    "best_par_rfrnest = gridsearchcv_rfr_n_est.best_params_\n",
    "print([res_rfrnest, best_est_rfrnest, best_par_rfrnest])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tuning models for balance between speed and performance\n",
    "\n",
    "# def tune_svsp(modeltune, xtu, ytu, max_time, param_to_adj, list_adj_vals, dict_const_params):\n",
    "#     print(str(modeltune))\n",
    "#     print(param_to_adj)\n",
    "#     mtime = 0\n",
    "#     xtutr, xtutest, ytutr, ytutest = train_test_split(xtu, ytu, random_state=2)\n",
    "#     tuning_results = pd.DataFrame(index=range(len(list_adj_vals)), columns=['param_val', 'tr_rmsle', 'test_rmsle', 'time'])\n",
    "#     for i, par_val in enumerate(list_adj_vals):\n",
    "#         if mtime > max_time:\n",
    "#             break\n",
    "#         dict_const_params[param_to_adj] = par_val\n",
    "#         mod = modeltune(**dict_const_params)\n",
    "#         timeA = time.time()\n",
    "#         mod.fit(xtutr, ytutr)\n",
    "#         pred_tr = mod.predict(xtutr)\n",
    "#         pred_test = mod.predict(xtutest)\n",
    "#         mtime = time.time() - timeA\n",
    "#         print(mtime)\n",
    "#         score_rmsle_tr = rmsle(ytutr, pred_tr)\n",
    "#         score_rmsle_test = rmsle(ytutest, pred_test)\n",
    "#         tuning_results.iloc[i] = {'param_val' : par_val, 'tr_rmsle' : score_rmsle_tr, 'test_rmsle' : score_rmsle_test,\n",
    "#                                  'time' : mtime}\n",
    "#     print(tuning_results)\n",
    "#     return tuning_results\n",
    "\n",
    "#Random Forest Regressor\n",
    "\n",
    "#tuning n_estimators; ~120 seems good\n",
    "\n",
    "# vals_n_est = [5,6,7,8,9,20,50]\n",
    "# dpar = {'max_depth' : 15, 'min_samples_split' : 2, 'max_leaf_nodes' : 50, 'max_features' : 'auto', 'random_state' : 2,\n",
    "#         'n_jobs' : 5, 'min_samples_leaf' : 0.15}\n",
    "# rfr_tune_nest_res = tune_svsp(RandomForestRegressor, xtr0, ytr, 8, 'n_estimators', vals_n_est, dpar)\n",
    "# vals_n_est = range(36,48)\n",
    "# dpar = {'max_depth' : 25, 'min_samples_split' : 0.4, 'max_leaf_nodes' : 50, 'max_features' : 0.3, 'random_state' : 2,\n",
    "#         'n_jobs' : 5, 'min_samples_leaf' : 0.15}\n",
    "# rfr_tune_nest_res = tune_svsp(RandomForestRegressor, xtr1, ytr, 8, 'n_estimators', vals_n_est, dpar)\n",
    "# vals_n_est = [5,7,9,20,50]\n",
    "# rfr_tune_nest_res = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'n_estimators', vals_n_est, dpar)\n",
    "# dpar = {'max_depth' : None, 'min_samples_split' : 2, 'max_leaf_nodes' : None, 'max_features' : 'auto', 'random_state' : 2,\n",
    "#         'n_jobs' : 5, 'min_samples_leaf' : 1}\n",
    "# vals_n_est = [130, 160, 190, 210]\n",
    "# rfr_tune_nest_res1 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'n_estimators', vals_n_est, dpar)\n",
    "# vals_n_est = [300, 400, 500]\n",
    "# rfr_tune_nest_res2 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'n_estimators', vals_n_est, dpar)\n",
    "# vals_n_est = [150, 170, 180]\n",
    "# rfr_tune_nest_res2 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'n_estimators', vals_n_est, dpar)\n",
    "# print([rfr_tune_nest_res, rfr_tune_nest_res1, rfr_tune_nest_res2])\n",
    "# vals_n_est = [100, 110, 120]\n",
    "# rfr_tune_nest_res3 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'n_estimators', vals_n_est, dpar)\n",
    "\n",
    "#tuning max_depth; ~16 seems good\n",
    "# dpar = {'n_estimators' : 130, 'min_samples_split' : 2, 'max_leaf_nodes' : None, 'max_features' : 'auto', 'random_state' : 2,\n",
    "#         'n_jobs' : 5, 'min_samples_leaf' : 1}\n",
    "# vals_md = [5, 10, 15, 100]\n",
    "# rfr_tune_md_res0 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'max_depth', vals_md, dpar)\n",
    "# vals_md = [13,14,15,16,17,25,40]\n",
    "# rfr_tune_md_res1 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'max_depth', vals_md, dpar)\n",
    "# print([rfr_tune_md_res0, rfr_tune_md_res1])\n",
    "# vals_md = range(18,22)\n",
    "# rfr_tune_md_res2 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'max_depth', vals_md, dpar)\n",
    "\n",
    "#tuning min_samples_split; around 10.0**-4 seems good\n",
    "# dpar = {'n_estimators' : 130, 'max_depth' : 16, 'max_leaf_nodes' : None, 'max_features' : 'auto', 'random_state' : 2,\n",
    "#         'n_jobs' : 5, 'min_samples_leaf' : 1}\n",
    "# vals_mss = [0.5,0.4,0.3,0.2]\n",
    "# rfr_tune_mss_res0 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'min_samples_split', vals_mss, dpar)\n",
    "# vals_mss = [0.15, 0.1, 0.05, 0.01]\n",
    "# rfr_tune_mss_res1 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'min_samples_split', vals_mss, dpar)\n",
    "# vals_mss = [0.005, 0.001, 2]\n",
    "# rfr_tune_mss_res2 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'min_samples_split', vals_mss, dpar)\n",
    "# vals_mss = [50,10,4,3]\n",
    "# rfr_tune_mss_res3 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'min_samples_split', vals_mss, dpar)\n",
    "# vals_mss = 5*(10.0**(-np.arange(4,8)))\n",
    "# rfr_tune_mss_res4 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'min_samples_split', vals_mss, dpar)\n",
    "# vals_mss = 10.0**(-4)*np.array([6,10,40,60,100,400])\n",
    "# rfr_tune_mss_res5 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'min_samples_split', vals_mss, dpar)\n",
    "# print([rfr_tune_mss_res0, rfr_tune_mss_res1, rfr_tune_mss_res2, rfr_tune_mss_res3, rfr_tune_mss_res4, rfr_tune_mss_res5])\n",
    "# vals_mss = 10.0**(-6)*np.array([6,10,30,40,60,70,100,400])\n",
    "# rfr_tune_mss_res6 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'min_samples_split', vals_mss, dpar)\n",
    "\n",
    "#tuning max_features; 0.8 seems good\n",
    "# dpar = {'n_estimators' : 130, 'max_depth' : 16, 'min_samples_split' : 10.0**(-4), 'max_leaf_nodes' : None, 'random_state' : 2,\n",
    "#         'n_jobs' : 5, 'min_samples_leaf' : 1}\n",
    "# vals_mf = [0.4, 0.6, 0.8, 'auto']\n",
    "# rfr_tune_mf_res0 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'max_features', vals_mf, dpar)\n",
    "# print(rfr_tune_mf_res0)\n",
    "# vals_mf = [0.65, 0.7, 0.75, 0.85, 0.9, 0.95]\n",
    "# rfr_tune_mf_res1 = tune_svsp(RandomForestRegressor, xtr0, ytr, 360, 'max_features', vals_mf, dpar)\n",
    "\n",
    "\n",
    "endtime = how_long(tstart)\n",
    "\n",
    "\n",
    "#Section B Part 2 version 2\n",
    "\n",
    "#trying RandomizedSearchCV for adjusting parameters\n",
    "rfr = RandomForestRegressor(n_estimators=20, max_depth=25, min_samples_split=0.4, max_leaf_nodes=50,\n",
    "                            max_features=0.3, random_state=2, n_jobs=5, min_samples_leaf=0.15)\n",
    "sgdr = linear_model.SGDRegressor(max_iter=7, random_state=2, alpha=1e-05)\n",
    "gbr = GradientBoostingRegressor(learning_rate=0.2, min_samples_leaf=50, max_features='sqrt',\n",
    "                                subsample=0.8, random_state=2, n_estimators=40, min_samples_split=0.6, max_depth=2)\n",
    "xgbr = xgb.XGBRegressor(random_state=2, subsample=0.7, max_depth=7)\n",
    "\n",
    "models = [[rfr, sgdr, gbr, xgbr]]\n",
    "models.append([copy.copy(m) for m in models[0]])\n",
    "\n",
    "rfr_param_grid = {'min_samples_leaf' : np.arange(0.05, 0.36, 0.05)}\n",
    "sgdr_param_grid = {'alpha': 10.0**-np.arange(1,7)}\n",
    "gbr_param_grid = {'n_estimators' : range(10, 61, 10),\n",
    "                  'max_depth' : range(2, 8),\n",
    "                 'min_samples_split' : np.arange(0.5, 0.95, 0.05)}\n",
    "xgbr_param_grid = {'max_depth' : [3, 5, 6, 7],\n",
    "                  'subsample' : np.arange(0.4, 1, 0.1)}\n",
    "\n",
    "par_grids = [rfr_param_grid, sgdr_param_grid, gbr_param_grid, xgbr_param_grid]\n",
    "best_models = pd.DataFrame(index=range(4), columns=range(2))\n",
    "predict_tr = best_models.copy()\n",
    "predict_test = best_models.copy()\n",
    "cvresults_randsearch = best_models.copy()\n",
    "\n",
    "#making RandomizedSearhCV\n",
    "def makegs(model_mgs, pg_mgs):\n",
    "    return RandomizedSearchCV(model_mgs, param_distributions = pg_mgs, n_iter=9, cv=3, n_jobs=5, random_state=2)\n",
    "\n",
    "#fitting Randomized Search\n",
    "def fitgs(gsfgs, xfgs, yfgs):\n",
    "    gsfgs.fit(xfgs, yfgs)\n",
    "    return gsfgs\n",
    "\n",
    "#making predictions\n",
    "def predbm(bmpbm, xtrpbm, xtestpbm):\n",
    "    ptr = bmpbm.predict(xtrpbm)\n",
    "    ptest = bmpbm.predict(xtestpbm)\n",
    "    return ptr, ptest\n",
    "\n",
    "tstart = how_long(tstart)\n",
    "\n",
    "#i=1 is for fewer features set, not i=0\n",
    "# for j in range(1,4):\n",
    "#     i=0\n",
    "#     gsearch = makegs(models[j], par_grids[j])\n",
    "#     gsearch = fitgs(gsearch, lxtr[i], ytr)\n",
    "#     tstart = how_long(tstart)\n",
    "#     print(gsearch.best_params_)\n",
    "#     for i in range(2):\n",
    "#         best_models.iloc[j, i] = gsearch.best_estimator_\n",
    "\n",
    "#without Randomized Search\n",
    "# for j in range(4):\n",
    "#     for i in range(2):\n",
    "#         best_models.iloc[j, i] = models[i][j]\n",
    "#         best_models.iloc[j, i].fit(lxtr[i], ytr)\n",
    "#         tstart = how_long(tstart)\n",
    "\n",
    "#with Randomized Search\n",
    "# print('Randomized Search CV')\n",
    "# for i in range(2):\n",
    "#     print('Data set {}'.format(i))\n",
    "#     for j in range(4):\n",
    "#         print('Model {}'.format(j))\n",
    "#         gsearch = makegs(models[i][j], par_grids[j])\n",
    "#         gsearch = fitgs(gsearch, lxtr[i], ytr)\n",
    "#         tstart = how_long(tstart)\n",
    "#         best_models.iloc[j, i] = gsearch.best_estimator_\n",
    "#         print(gsearch.best_params_)\n",
    "#         cvres = pd.DataFrame(gsearch.cv_results_)\n",
    "#         cvresults_randsearch.iloc[j, i] = cvres.to_string()\n",
    "#         print(cvres)\n",
    "\n",
    "# for i in range(2):\n",
    "#     for j in range(4):\n",
    "#         prtr, prtest = predbm(best_models.iloc[j, i], lxtr[i], lxtest[i])\n",
    "#         print(prtr)\n",
    "#         print(prtest)\n",
    "#         predict_tr.iloc[j, i] = prtr\n",
    "#         predict_test.iloc[j, i] = prtest\n",
    "#         tstart = how_long(tstart)\n",
    "\n",
    "\n",
    "#Section C: RMSE CV scores\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "#root mean squared error cross-validation scores\n",
    "def rmse_cv(model, x_rmsecv, y_rmsecv):\n",
    "    rmse = np.sqrt(-cross_val_score(model, x_rmsecv, y_rmsecv, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)\n",
    "\n",
    "scores_rmsecv = pd.DataFrame(index=range(4), columns=range(2))\n",
    "\n",
    "print('RMSE CV scores, fewer features models only')\n",
    "for j in range(4):\n",
    "    scores_rmsecv[1][j] = rmse_cv(best_models[1][j], xtr1, ytr)\n",
    "    print(scores_rmsecv[1][j])\n",
    "\n",
    "endtime = how_long(tstart)\n",
    "\n",
    "\n",
    "#Section D: model scores and RMSLE scores\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "#making some empty dataframes\n",
    "tr_scores_m = pd.DataFrame(index=range(4), columns=range(2))\n",
    "tr_scores_rmsle = tr_scores_m.copy()\n",
    "test_scores_m = tr_scores_m.copy()\n",
    "test_scores_rmsle = tr_scores_m.copy()\n",
    "\n",
    "#function for putting scores into dataframes\n",
    "def scores(model_sc, x_sc, y_sc, predict_sc):\n",
    "    score_m = model_sc.score(x_sc, y_sc)\n",
    "    score_rmsle = rmsle(y_sc, predict_sc)\n",
    "    return score_m, score_rmsle\n",
    "\n",
    "#going through models and printing scores\n",
    "for i in range(2):\n",
    "    print('Set {}'.format(i))\n",
    "    for j in range(4):\n",
    "        tr_scores_m[i][j], tr_scores_rmsle[i][j] = scores(best_models[i][j], lxtr[i], ytr, predict_tr[i][j])\n",
    "        test_scores_m[i][j], test_scores_rmsle[i][j] = scores(best_models[i][j], lxtest[i], ytest, predict_test[i][j])\n",
    "        print('Model {}'.format(j))\n",
    "        print('Training {}, {}'.format(tr_scores_m[i][j], tr_scores_rmsle[i][j]))\n",
    "        print('Test {}, {}'.format(test_scores_m[i][j], test_scores_rmsle[i][j]))\n",
    "\n",
    "endtime = how_long(tstart)\n",
    "\n",
    "\n",
    "#Section E: feature importances\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "#looking at feature importances\n",
    "def avpr_featimp(smodels, colnames):\n",
    "    df_f_imp = pd.DataFrame(index=colnames, columns=[0, 2, 3])\n",
    "    feat_imp = [m.feature_importances_ for m in smodels]\n",
    "    \n",
    "    df_f_imp.iloc[:] = np.transpose(feat_imp)\n",
    "    df_f_imp['average'] = df_f_imp.mean(axis=1)\n",
    "    print(df_f_imp)\n",
    "    print(df_f_imp['average'].loc[df_f_imp['average']>0.01])\n",
    "    return df_f_imp\n",
    "\n",
    "print('Feature importances')\n",
    "cm0 = best_models[0].iloc[[0, 2, 3]]\n",
    "f_imp0 = avpr_featimp(cm0, x0.columns)\n",
    "cm1 = best_models[1].iloc[[0, 2, 3]]\n",
    "f_imp1 = avpr_featimp(cm1, x1.columns)\n",
    "\n",
    "endtime = how_long(tstart)\n",
    "\n",
    "\n",
    "#Section F: stacking averaged models\n",
    "\n",
    "#copied from Kaggle kernel \"Stacked Regressions to predict House Prices\", by Serigne\n",
    "\n",
    "# class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "#     def __init__(self, models):\n",
    "#         self.models = models\n",
    "        \n",
    "#     # we define clones of the original models to fit the data in\n",
    "#     def fit(self, X, y):\n",
    "#         self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "#         # Train cloned base models\n",
    "#         for model in self.models_:\n",
    "#             model.fit(X, y)\n",
    "\n",
    "#         return self\n",
    "    \n",
    "#     #Now we do the predictions for cloned models and average them\n",
    "#     def predict(self, X):\n",
    "#         predictions = np.column_stack([\n",
    "#             model.predict(X) for model in self.models_\n",
    "#         ])\n",
    "#         return np.mean(predictions, axis=1)\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "                y_pred = instance.predict(X.iloc[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n",
    "#end of copied code\n",
    "\n",
    "# averaged_models = AveragingModels(models = (bests[0], bests[2], bests[3]))\n",
    "# averaged_models.fit(X_train, y_train)\n",
    "# pred_av = averaged_models.predict(X_train)\n",
    "# score_average = rmsle(y_train, pred_av)\n",
    "# print('Average:')\n",
    "# print(score_average)\n",
    "\n",
    "# test_pr_av = averaged_models.predict(X_test)\n",
    "# score_test_av = rmsle(y_test, test_pr_av)\n",
    "# print(score_test_av)\n",
    "\n",
    "# stacked_averaged_models = StackingAveragedModels(base_models = (best_models[0], best_estimators[2]),\n",
    "#                                                  meta_model = best_estimators[3])\n",
    "# stacked_averaged_models.fit(X_train, y_train)\n",
    "# pred_st_av = stacked_averaged_models.predict(X_train)\n",
    "# score_st_av = rmsle(y_train, pred_st_av)\n",
    "# print('Stacked average:')\n",
    "# print(score_st_av)\n",
    "\n",
    "# test_pr_st_av = stacked_averaged_models.predict(X_test)\n",
    "# score_test_st_av = rmsle(y_test, test_pr_st_av)\n",
    "# print(score_test_st_av)\n",
    "\n",
    "# stacked_averaged_models.fit(dfx, dfy)\n",
    "\n",
    "# pred_st_av = stacked_averaged_models.predict(dfx)\n",
    "\n",
    "# score_st_av = rmsle(dfy, pred_st_av)\n",
    "# print('Stacked average:')\n",
    "# print(score_st_av)\n",
    "\n",
    "# predictions_f = stacked_averaged_models.predict(test)\n",
    "# print(predictions_f)\n",
    "\n",
    "# df_s = pd.DataFrame(data=testRaw[['User_ID', 'Product_ID']], index=test.index)\n",
    "# df_s['Purchase'] = predictions_f\n",
    "# print(df_s)\n",
    "# df_s.to_csv('bfsubmission1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
